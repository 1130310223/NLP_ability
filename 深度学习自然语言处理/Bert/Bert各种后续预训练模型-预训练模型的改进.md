Bert各种后续预训练模型-预训练模型的改进

参考资料：

站在BERT肩膀上的NLP新秀们（PART II） - kaiyuan的文章 - 知乎
https://zhuanlan.zhihu.com/p/68362016

√ XLMs from Facebook

√ LASER from Facebook

√ MASS from Microsoft

√ UNILM from Microsoft

1. 邱锡鹏老师发表了关于NLP预训练模型的综述《Pre-trained Models for Natural Language Processing: A Survey》

这里有一个对这个的解读，写的非常好，在这个文章中，这个作者也列出来了自己的另外另个文章，可以看一看
NLP算法面试必备！史上最全！PTMs：NLP预训练模型的全面总结 - JayLou娄杰的文章 - 知乎
https://zhuanlan.zhihu.com/p/115014536
当然在这里文章里面，有一个链接，非常重要，就是对预训练模型单模型的精度，注意这里是精度，都是链接到了知乎文章
写的都是非常好！！！！！非常好，链接地址在这里https://github.com/loujie0822/Pre-trained-Models
这里链接一定要看

这里还有一个关于邱老师综述的解读，也很好
论文笔记 - NLP 预训练模型综述 - 徐阿衡的文章 - 知乎
https://zhuanlan.zhihu.com/p/139015428