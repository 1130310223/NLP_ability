Bert如何融入知识(一)-百度和清华ERINE

首先想一下Bert是如何训练的？首先我获取无监督语料，随机mask掉一部分数据，去预测这部分信息。

这个过程其实和W2C很类似，上下文相似的情况下，mask掉的单词的词向量很可能非常相近。

比如说”今天米饭真好吃“和”今天苹果真好吃“，很有可能”米饭“和”苹果“学出来的向量就很相似。

我在李如有一篇文章中《BERT句子表示的可视化》有这样一句话，contextual dependent词向量的一个缺点，就是上下文相似的情况下词向量也很接近。

从这里，我觉得很容易就可以发现一个问题，就是Bert确实抽取能力非常的强，但是他也是在死记硬背的学这些知识。

想一下，为什么我们需要在Bert中融入一些知识呢？

我们考虑这么一个例子，比如我要对一个文本进行分类：”库克今日来北京进行商务洽谈活动“

单从bert做一个文本分类，可能模型很难从语义角度进行决断。

但是，我现在的知识图谱中有这样一个三元组：库克-CEO-苹果公司

我把这个三元组的信息融入到我的模型之中，也就是我在文本分类的时候不仅仅使用了你的原始文本，还是使用了知识图谱中的三元组信息，相当于一种信息的增强，这个时候我的模型就可以文本分类为”IT公司“这个类别。

一般来说，涉及到Bert中融入知识，大家都会涉及到两个文章：百度的 ERNIE from Baidu 和清华的ERNIE from THU

我先从整体的思路说一下两者：

ERNIE from Baidu 出发点是这样的，Bert 的mask只是 mask掉单字，放在中文中，一般来说词汇会带有比字更多的信息。

比如说 

哈[mask]滨真冷啊 是Bert基础操作

[mask][mask][mask]真冷啊 是ERNIE from Baidu的操作

也就是，我预测的不仅仅是一个单字，而是一个实体词组。

对于这个操作，我是这么想的，首先从难度来讲，去预测一个词组会比预测一个单字难，而且这个词组是一个实体，所以在学习的时候回学习到实体信息


ERNIE from THU

对于这个模型，我是这么想的，百度利用的是预测句子中的实体信息。而清华这边的操作是加入了外部的知识信息。

就像最开始我们的例子，”库克-CEO-苹果公司“，这是外部知识，这个不是我文本中的信息，相当于显示的加入了外部信息。

当然清华这边应该也只是使用到了实体信息（做了实体对齐）

我们需要考虑两个问题：

1. 如何抽取并且更好的表达知识图谱的信息：知识嵌入算法（如TransE）

2. 实体向量和Bert的向量在不同的空间，如何缓解两者之间的Gap：

对于这个问题，从模型架构上来解决，使用两种：

textual encoder (T-Encoder)：类别Bert

knowledgeable encoder (K-Encoder)：用于将外部的知识图谱的信息融入到模型中；



对于Bert融入知识信息，主要是参考以下文章：

站在BERT肩膀上的NLP新秀们（PART I） - kaiyuan的文章 - 知乎
https://zhuanlan.zhihu.com/p/68295881

写的还不错，介绍了百度和清华的ERINE

Bert 改进： 如何融入知识 - 老宋的茶书会的文章 - 知乎
https://zhuanlan.zhihu.com/p/69941989

写的还不错，介绍了百度和清华的ERINE

BERT与知识图谱的结合——ERNIE模型浅析 - 段易通的文章 - 知乎
https://zhuanlan.zhihu.com/p/75466388

写的还不错，介绍了百度和清华的ERINE



