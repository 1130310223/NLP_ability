BERT模型蒸馏资源汇总：

主要是做BERT的蒸馏工作，之后要看的东西列举如下：

1. FastBert 自蒸馏模型，解读：

广告行业中那些趣事系列19：工业场景超实用的线上推理FastBERT - 数据拾光者的文章 - 知乎
https://zhuanlan.zhihu.com/p/208340002


推理怎么又快又稳？且看我FastBERT - 学术头条的文章 - 知乎
https://zhuanlan.zhihu.com/p/165006396

FastBERT：又快又稳的推理提速方法 - 李rumor的文章 - 知乎
https://zhuanlan.zhihu.com/p/127869267

2. 一个简单的论文汇总，一共15个 ：

15篇论文全面概览BERT压缩方法 - AI科技大本营的文章 - 知乎
https://zhuanlan.zhihu.com/p/93728391

3. 中间层loss一起学习：

Patient Knowledge Distillation for BERT Model Compression
https://paperswithcode.com/paper/patient-knowledge-distillation-for-bert-model

Bert之多层知识蒸馏 - 张雨石的文章 - 知乎
https://zhuanlan.zhihu.com/p/93639885

4. bert-of-theseus

模型压缩实践系列之——bert-of-theseus，一个非常亲民的bert压缩方法 - 邱震宇的文章 - 知乎
https://zhuanlan.zhihu.com/p/112787764

代码：https://github.com/JetRunner/BERT-of-Theseus