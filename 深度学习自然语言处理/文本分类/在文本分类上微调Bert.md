今天分享的论文主要是讲Bert如何在文本分类上获得比较好的效果，比较简单：[How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583, "How to Fine-Tune BERT for Text Classification?")：不涉及什么复杂公式，也比较早了，里面很多东西对于当下已经司空见惯，我就直接就分享论文结论，攒个思路。

# 1. 如何处理长文本

我比较感兴趣的是一点是Bert处理长文本的思路。

首先数据集是IMDB，文本分类任务，超过512个token的12.69%，最大长度为3045；

## 1.1 截断方法：

1. 保留头部：保留头部最开始的510个tokens
2. 保留尾部：保留最后的610个tokens
3. 头部加尾部：头部128+尾部382

## 1.2 分层的方法：

简单来说就是把文本分为 k = L/510个小段落，每个都喂进去Bert，然后得到的K个【CLS】的输出向量，我们对这个K个向量做：

1. mean pooling
2.  max pooling 
3. self-attention

直接看结果：

![Bert处理长文本](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-070900.png)

看结果，我们知道，头部加尾部会获得更好的结果。

## 2. 其他结论

1. BERT 顶层对于文本分类任务更加有效
2. **每层适当的逐层降低学习速率**，可以提高文本分类效果
3. 任务内和领域内（和任务内数据分布相似）的进一步预训练可以提升文本分类效果

对于第二点，降低学习率来说，论文中是从顶层到底层逐渐降低，越靠近输出学习率越高，越靠近输入层，学习率越低，这一点还是挺有意思的。

对于第三点，任务内数据和领域内数据，对提升效果都有用，通用领域基本没啥用，因为Bert本来就是在通用领域训练的。

还有意思的一点是，并不是在任务内的数据训练的越多step越好，直接看图：

![领域内数据进一步预训练](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-070838.png)

也就是说，在任务领域数据预训练可以提升效果，但是也有注意预训练的步数，不能是过分（有点过拟合的感觉，但是感觉说过拟合有点不准确）。

# 3. 总结

掌握以下几点：

1. 如何处理长文本：head/tail/combine two
2. 不同层不同学习率提升效果，越靠近输入层学习率应该越低
3. 领域内和任务内数据进一步预训练提升效果：**注意进一步预训练步数控制**

参考链接：

文本 × 分类：让 BERT 适配短句分类任务 - 小莲子的文章 - 知乎 https://zhuanlan.zhihu.com/p/148501319