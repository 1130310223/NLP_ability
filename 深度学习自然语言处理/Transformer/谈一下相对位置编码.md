谈一下相对位置编码

正余弦函数表示的位置信息在经过attention中的线性变化之后，相对位置信息消失，所以需要优化。

一般来讲，谈到相对位置编码，三种比较有名：RPR； Transformer-XL；complex embeddings；

这一篇简单讲一下RPR。

RPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息，改进的话就是不再输入的时候
进行位置编码，而是在attention中显示把相对位置信息加入进去。

窗口为4，也就是包含自己本身，需要学习9个相对embedding。

每个单词对应的时候，自己都是代表i，也就是最中间的那个。

需要好多图，之后再写吧

列一下参考资料：
[看这里](https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8E%9F%E7%89%88Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%A9%B6%E7%AB%9F%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8C%85%E5%90%AB%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF.md)

https://mp.weixin.qq.com/s/NPM3w7sIYVLuMYxQ_R6PrA

图还不错，值得看看